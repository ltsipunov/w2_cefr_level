{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac97ec8e",
   "metadata": {},
   "source": [
    "# Data Collector \n",
    "Для повышения модульности проект разбит на 2 блокнота - для сбора данных в датасет и собственно модель для работы с датасетом.  \n",
    "Этот блокнот содержит сборщик данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d0a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import nltk\n",
    "import nltk.tokenize as nt\n",
    "import nltk.corpus as nc\n",
    "import nltk.stem as ns\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "import pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89aa6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c8c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    text =\"\"\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.text+=data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b673bd",
   "metadata": {},
   "source": [
    "Словарь загружается из текстоых файлов, каждый файл содержит словарные статьи с указанием частей речи.  \n",
    "Словарь должен содержать слова одной категории, категория указывается при загрузке файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "#    part_of_speech = ['n.','v.','adj.','conj.','prep.','pron.']\n",
    "#    template = parts_of_speech.join('')\n",
    "    \n",
    "    def __init__(self, filenames):\n",
    "        self.words = {}\n",
    "        for fn,grade in filenames.items():\n",
    "            with open(fn) as f:\n",
    "                for ln in f:\n",
    "                    w,set_g = self.split_line(ln)\n",
    "                    self.insert(word = w,grade = grade, grammar= set_g )\n",
    "                            \n",
    "    def split_line(self,ln):\n",
    "        template='((n)|(v)|(prep)|(pron)|(conj)|(adj)|(adv))[\\.\\,]?'\n",
    "        tokens =ln.split(' ')\n",
    "        word = tokens[0]\n",
    "        gram = set()\n",
    "        for t in tokens[1:]:\n",
    "            m = re.match(template,t) \n",
    "            if m:\n",
    "                gram.add(m[1])\n",
    "        return word,gram\n",
    "                            \n",
    "    def insert(self, word, grade, grammar):\n",
    "        ww = self.words.pop(word,{})\n",
    "        gr = ww.pop('grammar',set())\n",
    "        gr |= grammar\n",
    "        ww['grammar'] =  gr\n",
    "        g = ww.pop('grade',set())\n",
    "        g.add(grade)\n",
    "        ww['grade']=g\n",
    "        self.words[word] = ww \n",
    "        \n",
    "    def summary(self):\n",
    "        count= 0\n",
    "        grades = {}\n",
    "        grammars = {}\n",
    "        for v in self.words.values():\n",
    "            if not str(v['grammar']) in grammars:\n",
    "                grammars[str(v['grammar'])] = 0 \n",
    "            grammars[str(v['grammar'])]+=1\n",
    "            if not str(v['grade']) in grades:\n",
    "                grades[str(v['grade'])] = 0 \n",
    "            grades[str(v['grade'])]+=1\n",
    "            count+=1    \n",
    "        return count,grades,grammars    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34575544",
   "metadata": {},
   "source": [
    "Простая обёртка для загрузки и чистки стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608f8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stops:\n",
    "    def __init__(self ):\n",
    "        self.stop_words = set(nc.stopwords.words(\"english\")) | {\"'\"}\n",
    "    \n",
    "    def purge(self,words): \n",
    "        return(  [ w for w in words if w.casefold() not in self.stop_words ] )\n",
    "\n",
    "stops = Stops()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae8748",
   "metadata": {},
   "source": [
    "Класс для сбора собственных имён, при анализе частота собственных имён является отдельным признаком "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22900c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NE_Chunks:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def extract(self,words):\n",
    "        tags = nltk.pos_tag(words)\n",
    "        tree = nltk.ne_chunk(tags, binary=True)\n",
    "        ne =  set(\n",
    "                \" \".join(i[0] for i in t)\n",
    "                for t in tree\n",
    "                if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "            )\n",
    "        common = words #[ w for w in words if w  not in ne ]  \n",
    "        return(ne,common)\n",
    "    \n",
    "ne_chunks = NE_Chunks()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a14e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "На прикидках LancasterStemmer показал полное превосходство над остальными, не нашёл, в чём он может уступать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03645573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = SnowballStemmer('english')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd79b2",
   "metadata": {},
   "source": [
    "Класс для сбора статистики с файлов с субтитрами.  \n",
    "Подсчитывает частотность слов каждой категории по использванию и по доле в словаре.   \n",
    "Также вычисляется общее количество слов, средняя дляни слова, частотность собственных имен.\n",
    "Порядка 30 % слов не попадает в подсчет (баг пока не найден), но по этой причене можно считать, что частотности категорий не коррелируют между собой :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9c5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subfile():\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.text = ''\n",
    "        self.ne ={}\n",
    "        self.freq = dict()\n",
    "        self.summary = dict()\n",
    "        self.tokens = list()\n",
    "        file_name_parts = filename.split('/')[-1].split('\\\\')[-1].split('.')\n",
    "        self.movie = '.'.join( file_name_parts[:-1])                           # w/o 'srt'\n",
    "        self.misses = {}\n",
    "#        print(self.movie)\n",
    "        self.load(filename)\n",
    "    \n",
    "    def load(self,filename):\n",
    "        encs = ['utf-8','iso-8859-1']\n",
    "        last_error = None\n",
    "        for en in encs:\n",
    "            try:\n",
    "                srt = pysrt.open(filename, encoding=en )\n",
    "            except UnicodeDecodeError as e:\n",
    "                last_error = e\n",
    "                srt = []\n",
    "            if srt :\n",
    "                break\n",
    "        \n",
    "        if not srt:\n",
    "            raise last_error \n",
    "        h=MyHTMLParser()\n",
    "        h.feed(srt.text)\n",
    "        self.text = h.text         \n",
    "    \n",
    "    def parse(self):\n",
    "#        self.tokens = nt.word_tokenize(self.text)\n",
    "        self.tokens = stops.purge( self.tokenize(self.text) )\n",
    "        self.ne,self.tokens =  ne_chunks.extract(self.tokens)\n",
    "        self.tokens = [w.lower() for w in self.tokens ]\n",
    "        self.ne = {w.lower() for w in self.ne }\n",
    "        self.summary = nltk.FreqDist(self.tokens)\n",
    "        \n",
    "    def tokenize(self,text):\n",
    "        rt = RegexpTokenizer(r\"\\w+|\\'\")\n",
    "        return( rt.tokenize(text) ) \n",
    "    \n",
    "    \n",
    "    def classify(self,vocab):\n",
    "        def find_word_grade(w):\n",
    "            gr = {}\n",
    "            try:\n",
    "                if w in vocab.words  : \n",
    "                    gr = max(vocab.words[w]['grade'])\n",
    "                elif  (stemmer.stem(w) in vocab.words):    \n",
    "                    gr= max(vocab.words[stemmer.stem(w)]['grade'])\n",
    "            except ValueError as ve: \n",
    "                print(w,ve)\n",
    "                \n",
    "            return(gr) \n",
    "    \n",
    "        def inc_dict_at( d,i,n):\n",
    "            g = d.get(i,0)\n",
    "            d[i] = g+n\n",
    "  \n",
    "                                      \n",
    "        cnt = 0\n",
    "        cnt_ne = 0\n",
    "        sum_length = 0 \n",
    "        nums = dict()\n",
    "        self.misses = {}\n",
    "        ids = dict()\n",
    "        g = {}\n",
    "        for w,n in self.summary.items():\n",
    "#             print(w)\n",
    "            g = find_word_grade(w)\n",
    "            if bool(g):\n",
    "                inc_dict_at( nums,g,n)\n",
    "                inc_dict_at( ids, g,1)\n",
    "            elif w in self.ne:\n",
    "                cnt_ne += 1\n",
    "            else:    \n",
    "                self.misses[w]=n                                     \n",
    "            cnt += n \n",
    "            sum_length += len(w)\n",
    "\n",
    "        self.freq = {  k+'_freq':round(v/cnt,4) for (k,v)  in nums.items() }\n",
    "        wc = len( self.summary ) \n",
    "        self.freq |= { k+'_rate':round(v/wc,4) for (k,v)  in ids.items() }\n",
    "        self.freq |= { 'qty': round(cnt), 'ne': round(cnt_ne/cnt,4), 'avg_length':round(sum_length/cnt,1) }\n",
    "\n",
    "    def statistic(self):\n",
    "        return({'movie':self.movie} | self.freq )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb78e4c",
   "metadata": {},
   "source": [
    "Сборщик статистики по всем файлам , сохраняет результаты в датасет в каталоге dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6044d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "\n",
    "    def __init__(self): #, srt_pattern , scores_path)\n",
    "#         self.srt_pattern= srt_pattern\n",
    "#         self.xls_path = scores_path \n",
    "        self.process()\n",
    "    \n",
    "    def get_levels(self,xls_path):\n",
    "        df = pd.read_excel(xls_path)\n",
    "        df = df.drop('id',axis=1)\n",
    "        df.columns = ['movie','level'] \n",
    "        resolve_levels = {'A2/A2+':'A2' ,'A2/A2+':'A2','A2/A2+, B1':'A2','B1, B2':'B1' }\n",
    "        df['level'] = df.level.apply(lambda l: resolve_levels.get( l.strip(),l.strip() ) )\n",
    "        return df\n",
    "\n",
    "    def gather_stats(self,file_pattern,vocab):\n",
    "        def add_one(acc, new ):\n",
    "            for k in acc:\n",
    "                if k not in new:\n",
    "                    new[k]=0\n",
    "            for k,v in new.items():\n",
    "                if k in acc:\n",
    "                    acc[k].append(v)\n",
    "                else:\n",
    "                    acc[k] = [v]\n",
    "\n",
    "        acc = {}\n",
    "        for fn in glob.glob(file_pattern):\n",
    "            try:\n",
    "                sf= Subfile(fn)        \n",
    "                sf.parse()\n",
    "            except UnicodeDecodeError as e:\n",
    "        #        print(sf.movie,e)\n",
    "                continue\n",
    "            sf.classify(vocab = vocab)\n",
    "            st = sf.statistic()\n",
    "            add_one(acc,st)\n",
    "        #    print(acc.keys(), st.keys())\n",
    "        return( pd.DataFrame.from_dict( acc ) ) \n",
    "    \n",
    "    def join_data(self, ds , df ):\n",
    "        dx = ds.merge(df,how='outer')\n",
    "        return(dx)\n",
    "        \n",
    "    def write(self, dx ):\n",
    "        dx.to_csv('datasets/movies.csv')\n",
    "        with open(\"movies_params.py\", \"w\") as w:\n",
    "            w.write(\"\"\"\\\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "        \"\"\" )\n",
    "            w.write(\"\\n\")\n",
    "            w.write(f\"feature_cols = {self.feature_cols(dx)} \\n\")\n",
    "            w.write(f\"target_col = 'level'\\n\")   \n",
    "            \n",
    "    def feature_cols(self,dx):\n",
    "        return( [c for c in dx.columns if c != 'movie'] )\n",
    "            \n",
    "    def process(self):\n",
    "        vocab =  Vocabulary({ 'Oxford_CEFR_level/A1.txt':'A1',\n",
    "                       'Oxford_CEFR_level/A2.txt':'A2',\n",
    "                      'Oxford_CEFR_level/B1.txt':'B1', \n",
    "                      'Oxford_CEFR_level/B2.txt':'B2',\n",
    "                      'Oxford_CEFR_level/C1.txt':'C1'   } ) \n",
    "        df = self.get_levels('English_scores/movies_labels.xlsx')\n",
    "        ds = self.gather_stats( 'English_scores/Subtitles_all/*/*.srt', vocab )\n",
    "        dx = self.join_data(ds,df)\n",
    "        self.write(dx)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee6dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 5s\n",
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DataCollector at 0x1cf95047670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DataCollector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
